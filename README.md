### Hi ðŸ‘‹ I'm Liang Yiwen
![IceEnd's GitHub stats](https://github-immortality.vercel.app/api?username=vitalityman)

I am diving deep into the intersection of hardware and AI, where the synergy between computational architecture and intelligent algorithms unlocks unprecedented efficiency and performance. My expertise spans two key areas:

- **1) High-Performance LLM Serving & Acceleration**: I specialize in accelerating inference speed and maximizing system throughput using advanced frameworks like vLLM and SGLang. My work focuses on optimizing distributed pipelines and request scheduling to ensure high-performance execution in large-scale environments.

- **2) Hardware-Aware Optimization & Quantization**: I focus on enhancing model efficiency on diverse hardware (including FPGAs and CPUs) by leveraging quantization algorithms and custom operator tuning. Using tools like llama.cpp and MLC-LLM, I implement compiler-assisted strategies to achieve low-latency, resource-efficient deployment.
