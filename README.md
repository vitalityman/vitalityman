### Hi ðŸ‘‹ I'm Liang Yiwen
![IceEnd's GitHub stats](https://github-immortality.vercel.app/api?username=vitalityman)

I am diving deep into the intersection of hardware and AI, focusing on the synergy between computational architecture and intelligent algorithms. My work centers on two key areas:

1) LLM Inference Acceleration: I specialize in high-throughput, low-latency deployment across distributed systems. My experience includes optimizing multi-batch request processing in vLLM and implementing Prefill-Decode (PD) separation in SGLang, utilizing RDMA for efficient inter-GPU communication.

2) High-Performance Model Optimization: I focus on maximizing efficiency on heterogeneous hardware (e.g., FPGAs) through end-to-end tuning. Specifically, I have optimized non-linear layer operators for llama.cpp on FPGAs and leveraged TVM within MLC-LLM to perform operator fusion adapted for FPGA architectures.
